{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../\")\n",
    "from bert_embedding import *\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SEC filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_link =\"https://github.com/juand-r/entity-recognition-datasets/tree/master/data/SEC-filings/CONLL-format/data/test/FIN3\"\n",
    "train_link = \"https://github.com/juand-r/entity-recognition-datasets/tree/master/data/SEC-filings/CONLL-format/data/train/FIN5.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/SEC-filings/CONLL-format/data/test/FIN3.txt -P ../data/sec_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/SEC-filings/CONLL-format/data/train/FIN5.txt -P ../data/sec_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/BTC/CONLL-format/data/h.conll -P ../data/ner_btc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/wikigold/CONLL-format/data/wikigold.conll.txt -P ../data/ner_wikigold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_ner_data(path, separator = \" \"):\n",
    "    with open(path) as f:\n",
    "        text = f.read().split(\"\\n\\n\")\n",
    "\n",
    "    output = []\n",
    "    for line in text:\n",
    "        feature_label = []\n",
    "        line = line.split(\"\\n\")\n",
    "        for entry in line:\n",
    "            feature_label.append(tuple(entry.split(separator)))\n",
    "        output.append(feature_label)\n",
    "    return output\n",
    "\n",
    "# get words and tags\n",
    "def unique_words_tags(data):\n",
    "    unique_words = []\n",
    "    unique_tags = []\n",
    "    for sent in data:\n",
    "        unique_words.extend(list(set(np.array(sent)[:,0])))\n",
    "        unique_tags.extend(list(set(np.array(sent)[:,-1])))\n",
    "        \n",
    "    return set(unique_words), set(unique_tags)\n",
    "\n",
    "# get words and tags distributions\n",
    "def distributions_words_tags(data):\n",
    "    unique_words = {}\n",
    "    unique_tags = {}\n",
    "    for i in range(len(data)-1):\n",
    "        sent = data[i]\n",
    "        for t in sent:\n",
    "            word = t[0]\n",
    "            tag = t[-1]\n",
    "            \n",
    "            if word in unique_words:\n",
    "                unique_words[word] += 1\n",
    "            else:\n",
    "                unique_words[word] = 1\n",
    "                \n",
    "            if tag in unique_tags:\n",
    "                unique_tags[tag] += 1\n",
    "            else:\n",
    "                unique_tags[tag] = 1\n",
    "                \n",
    "    return unique_words, unique_tags\n",
    "\n",
    "def sent_to_tuple(sent):\n",
    "    ner_tags = sent['ner_tags']\n",
    "    pos_tags = sent['pos_tags']\n",
    "    tokens = sent['tokens']\n",
    "    sent_list = []\n",
    "    for i in range(len(sent['ner_tags'])):\n",
    "        sent_list.append((tokens[i], pos_list[pos_tags[i]], label_list[ner_tags[i]]))\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/Users/yuchen.zhang/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset['train'].features['ner_tags'].feature.names\n",
    "pos_list = dataset['train'].features['pos_tags'].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_tuple(sent):\n",
    "    ner_tags = sent['ner_tags']\n",
    "    pos_tags = sent['pos_tags']\n",
    "    tokens = sent['tokens']\n",
    "    sent_list = []\n",
    "    for i in range(len(sent['ner_tags'])):\n",
    "        sent_list.append((tokens[i], pos_list[pos_tags[i]], label_list[ner_tags[i]]))\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'NNP', 'B-ORG'),\n",
       " ('rejects', 'VBZ', 'O'),\n",
       " ('German', 'JJ', 'B-MISC'),\n",
       " ('call', 'NN', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('boycott', 'VB', 'O'),\n",
       " ('British', 'JJ', 'B-MISC'),\n",
       " ('lamb', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_tuple(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = [sent_to_tuple(dataset['train'][x]) for x in range(len(dataset['train']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EU', 'NNP', 'B-ORG'),\n",
       " ('rejects', 'VBZ', 'O'),\n",
       " ('German', 'JJ', 'B-MISC'),\n",
       " ('call', 'NN', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('boycott', 'VB', 'O'),\n",
       " ('British', 'JJ', 'B-MISC'),\n",
       " ('lamb', 'NN', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_tags(conll)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = load_ner_data(\"/Users/yuchen.zhang/Documents/Projects/domain-adaptation-nlp/data/ner_tech/tech_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(sent):\n",
    "    return [(t[0], re.sub(\"E-\",\"I-\",re.sub(\"S-\",\"B-\",t[1]))) for t in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech = [transform_label(x) for x in tech]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_tags(tech)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_path = \"../data/ner_sec/FIN5.txt\"\n",
    "sec = load_ner_data(sec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(sec))\n",
    "# sec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_tags(sec)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9345\n"
     ]
    }
   ],
   "source": [
    "btc = []\n",
    "for data in [\"a\", \"b\", \"e\", \"f\", \"g\", \"h\"]:\n",
    "    btc.extend(load_ner_data(\"../data/ner_btc/\" + data + \".conll\", \"\\t\"))\n",
    "print(len(btc))\n",
    "# btc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'', 'B-LOC', 'B-ORG', 'B-PER', 'I-LOC', 'I-ORG', 'I-PER', 'O'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_tags(btc)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "wiki = load_ner_data(\"../data/ner_wikigold/\" + \"wikigold\" + \".conll.txt\", \" \")\n",
    "print(len(wiki))\n",
    "print(wiki[1][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 170524, 'I-ORG': 10001, 'I-MISC': 4556, 'I-PER': 11128, 'I-LOC': 8286, 'B-LOC': 11, 'B-MISC': 37, 'B-ORG': 24}\n",
      "{'I-MISC': 1392, 'O': 32721, 'I-ORG': 1958, 'I-PER': 1634, 'I-LOC': 1447}\n",
      "{'O': 39485, 'I-ORG': 384, 'I-LOC': 356, 'I-PER': 783, 'I-MISC': 7}\n",
      "{'O': 131814, 'B-LOC': 2822, 'B-PER': 7928, 'B-ORG': 4135, 'I-ORG': 1176, 'I-PER': 1554, 'I-LOC': 958, '': 5}\n"
     ]
    }
   ],
   "source": [
    "for data in [conll2003, wiki, sec, btc]:\n",
    "    print(distributions_words_tags(data)[1])\n",
    "# only wiki and sec works here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in conll:\n",
    "    for t in sent:\n",
    "        if t[-1] ==\"\":\n",
    "            print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels1(sent):\n",
    "    return [t[-1] for t in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2labels(wiki[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'O']"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2labels1(wiki[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    f = tokenize_encode_bert_sentences_sample(tokenizer_d, model_d, word)[0]\n",
    "    features = {}\n",
    "    for j in range(len(f)):\n",
    "        features[str(j)] = f[j]\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [sent2labels1(s) for s in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [sent2features(s) for s in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = tokenize_encode_bert_sentences_sample(tokenizer_d, model_d, \"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('/', 'NN', '-', 'O'),\n",
       "  ('s', 'NNS', '-', 'O'),\n",
       "  ('/', ':', '-', 'O'),\n",
       "  ('Bing', 'VBG', '-', 'I-PER'),\n",
       "  ('Yu', 'NNP', '-', 'I-PER')],\n",
       " [('',)]]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sec[-2:]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haha[0]\n",
    "# haha_f = {}\n",
    "# for j in range(len(haha[0])):\n",
    "#     haha_f[str(j)] = haha[0][j]\n",
    "# haha_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n",
    "https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_d = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_d = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8506 3513 10694\n"
     ]
    }
   ],
   "source": [
    "words_wiki, tags = unique_words_tags(wiki)\n",
    "words_sec, tags = unique_words_tags(sec)\n",
    "print(len(words_wiki), len(words_sec), len(words_wiki | words_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 413\n"
     ]
    }
   ],
   "source": [
    "sent_lens = [len(x) for x in wiki]\n",
    "print(max([len(x) for x in wiki]), max([len(x) for x in sec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "words_wiki.remove(\"\")\n",
    "words_sec.remove(\"\")\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words_wiki | words_sec)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_wiki = [to_categorical(i, num_classes=len(tags)) for i in y]\n",
    "y_wiki[0][0]\n",
    "\n",
    "y_sec = [to_categorical(i, num_classes=len(tags)) for i in y]\n",
    "y_sec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_wiki, test_wiki = train_test_split(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3340"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list.index(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ner_corpus = tokenize_encode_bert_sentences(tokenizer_d, model_d, words_list, \"../data/all_bert/encoded_ner_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24184825, -0.14191352,  0.09101695, -0.10449045, -0.07760043,\n",
       "       -0.04267633,  0.00831679,  0.27026916, -0.18425259, -0.22167444],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_encode_bert_sentences_sample(tokenizer_d, model_d, [\"The\"])[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30622771, -0.13204339,  0.0283046 , -0.07296999, -0.01897933,\n",
       "       -0.09043107,  0.10871619,  0.03657816, -0.14900076, -0.30661121])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ner_corpus[word2idx[\"The\"]-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_bert = np.load(\"../data/all_bert/encoded_ner_corpus.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    }
   ],
   "source": [
    "if (ner_bert[word2idx[\"Randall\"]-1][:10] != encoded_ner_corpus[word2idx[\"Randall\"]-1][:10]).any():\n",
    "    print(\"hha\")\n",
    "else:\n",
    "    print(\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3340"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(words_wiki | words_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10693, 10693)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sort()\n",
    "len(words), len(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import numpy as np\n",
    ">>> from sklearn.multiclass import OneVsRestClassifier\n",
    ">>> from sklearn.svm import SVC\n",
    ">>> X = np.array([\n",
    "...     [10, 10],\n",
    "...     [8, 10],\n",
    "...     [-5, 5.5],\n",
    "...     [-5.4, 5.5],\n",
    "...     [-20, -20],\n",
    "...     [-15, -20]\n",
    "... ])\n",
    ">>> y = np.array([0, 0, 1, 1, 2, 2])\n",
    ">>> clf = OneVsRestClassifier(SVC(kernel=\"linear\")).fit(X, y)\n",
    ">>> clf.predict([[-19, -20], [9, 9], [-5, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13738441 0.04755614]]\n",
      "[[-0.22718445  0.21206928]]\n",
      "[[-0.02586172 -0.0686952 ]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(clf.estimators_[i].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import BertTokenizer, BertModel, TokenClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_cased = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model_cased = BertModel.from_pretrained('bert-base-cased')\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 146, 1821, 2816, 102]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ner([\"I am happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  146, 1821, 2816,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer_cased([\"I am happy\"], return_tensors='pt', truncation=True, padding=True)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3082,  0.1167, -0.0885,  ..., -0.0279,  0.3513, -0.1974],\n",
       "         [ 0.1656, -0.2242,  0.1945,  ...,  0.3512,  0.0907,  0.4365],\n",
       "         [ 0.1831,  0.0775, -0.1243,  ...,  0.3164, -0.1820,  0.2935],\n",
       "         [ 0.2433,  0.0329,  0.1429,  ...,  0.4797,  0.1774,  0.1627],\n",
       "         [ 0.5956,  0.4143, -0.5277,  ...,  0.2868,  0.6934, -0.1289]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cased(**encoded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1893, -0.3310,  0.7314,  ...,  0.4139,  0.4107,  0.1432],\n",
       "         [-0.2228, -0.4651,  1.3479,  ...,  0.3291, -0.1585,  0.7715],\n",
       "         [-0.2823, -0.3869,  1.1078,  ...,  0.4386,  0.1224,  0.4917],\n",
       "         [-0.1279, -0.4046,  0.9786,  ...,  0.3558,  0.1769,  0.4220],\n",
       "         [-0.8327, -0.3331,  0.8985,  ..., -0.5422,  0.1240, -0.2639]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ner.bert(**encoded)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NER BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\").bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'happy', '[SEP]']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer_ner(\"happy\", return_tensors='pt', truncation=True, padding=True)\n",
    "output = model_ner(**encoded_input)[0]\n",
    "tokenizer_ner.convert_ids_to_tokens(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'happily', '[SEP]']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input1 = tokenizer_ner(\"happily\", return_tensors='pt', truncation=True, padding=True)\n",
    "output1 = model_ner(**encoded_input)[0]\n",
    "tokenizer_ner.convert_ids_to_tokens(encoded_input1['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'happy', '[SEP]']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input_cased = tokenizer_cased(\"happy\", return_tensors='pt', truncation=True, padding=True)\n",
    "output_cased = model_cased(**encoded_input_cased)[0]\n",
    "tokenizer_cased.convert_ids_to_tokens(encoded_input_cased['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'happily', '[SEP]']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input_cased1 = tokenizer_cased(\"happily\", return_tensors='pt', truncation=True, padding=True)\n",
    "output_cased1 = model_cased(**encoded_input_cased)[0]\n",
    "tokenizer_cased.convert_ids_to_tokens(encoded_input_cased1['input_ids'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
