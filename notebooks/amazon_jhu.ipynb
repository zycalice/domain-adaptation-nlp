{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Amazon JHU Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/yuchen.zhang/Documents/Projects/domain-adaptation-nlp/data/amazon_jhu/\"\n",
    "types = [\"books\", \"dvd\", \"electronics\", \"kitchen_&_housewares\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = f.read().decode(\"utf-8\", \"ignore\")\n",
    "    return [x.split(\"<review_text>\\n\")[1] for x in data.split(\"\\n</review_text>\") if len(x.split(\"<review_text>\\n\"))==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for t in types:\n",
    "    data[t] = {\"pos\":open_files(path + \"sorted_data/\" + t + \"/positive.review\"), \n",
    "               \"neg\":open_files(path + \"sorted_data/\" + t + \"/negative.review\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'amazon_jhu4_text.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac39ca0a63c846538ac6610c72e88914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcdbd675a624ba7b6da6a6b1f4506be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138611e47be4487486f1491e852ca89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173acb806e934ce7a96426363ec82951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd411a74c644ce88a6bf97b6f18c4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_encode_bert_sentences_batch(tokenizer, model, input_sentences, output_path, layer_type=\"CLS\"):\n",
    "    output = np.zeros([len(input_sentences), 768])\n",
    "    for i, x in enumerate(input_sentences):\n",
    "        output[i] = tokenize_encode_bert_sentences_sample(tokenizer=tokenizer, model=model,\n",
    "                                                          input_sentences=[x],\n",
    "                                                          layer_type=layer_type)\n",
    "    if output_path is not None:\n",
    "        np.save(output_path, output)\n",
    "    return output\n",
    "\n",
    "def tokenize_encode_bert_sentences_sample(tokenizer, model, input_sentences, layer_type=\"CLS\"):\n",
    "    encoded_input = tokenizer(input_sentences, return_tensors='pt', truncation=True, padding=True)\n",
    "    output = model(**encoded_input)[0]\n",
    "    if layer_type == \"CLS\":\n",
    "        output = output[:, 0, :].detach().numpy()\n",
    "    elif layer_type == \"first subtoken\":\n",
    "        output = output[:, 1, :].detach().numpy()\n",
    "    else:\n",
    "        output = output.detach().numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = {}\n",
    "for t in data:\n",
    "    embeds[t] = {\"pos\": tokenize_encode_bert_sentences_batch(tokenizer, model, data[t][\"pos\"], None, layer_type=\"CLS\"),\n",
    "                \"neg\": tokenize_encode_bert_sentences_batch(tokenizer, model, data[t][\"neg\"],None, layer_type=\"CLS\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'amazon_jhu4_roberta.pickle', 'wb') as handle:\n",
    "    pickle.dump(embeds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
